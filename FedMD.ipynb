{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np   \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from utils.sampling import mnist_iid, partition_public, partition_data_dataset\n",
    "from utils.options import args_parser\n",
    "from models.Update import DatasetSplit\n",
    "from models.test import test_img\n",
    "from models.resnet_client import resnet20, resnet16, resnet8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # parse args\n",
    "    args = args_parser(args=['--dataset','mnist', '--momentum','0.9', '--alpha','1',\n",
    "                             '--epochs','50', '--gpu','0', '--public_data_ratio','0.1', '--lr','0.01'])\n",
    "\n",
    "    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "    print('torch.cuda:',torch.cuda.is_available())\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and split users\n",
    "if __name__ == '__main__':\n",
    "    if args.dataset == 'mnist':\n",
    "        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda img: img.expand(3, -1, -1)), \n",
    "                                                    transforms.Normalize((0.1307,0.1307,0.1307), (0.3081,0.3081,0.3081))])\n",
    "        dataset_train = datasets.MNIST('data/mnist/', train = True, download = False, transform = trans_mnist)\n",
    "        dataset_test = datasets.MNIST('data/mnist/', train = False, download = False, transform = trans_mnist)\n",
    "\n",
    "        dataset_train, dataset_public = partition_public(dataset_train, args.public_data_ratio)\n",
    "        print('len(dataset_public): ', len(dataset_public))\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "    elif args.dataset == 'fashionmnist':\n",
    "        trans_fashionmnist = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda img: img.expand(3, -1, -1)), \n",
    "                                                    transforms.Normalize((0.1307,0.1307,0.1307), (0.3081,0.3081,0.3081))])\n",
    "        dataset_train = datasets.FashionMNIST('data/fashionmnist/', train = True, download = False, transform = trans_fashionmnist)\n",
    "        dataset_test = datasets.FashionMNIST('data/fashionmnist/', train = False, download = False, transform = trans_fashionmnist)\n",
    "    \n",
    "        dataset_train, dataset_public = partition_public(dataset_train, args.public_data_ratio)\n",
    "        print('len(dataset_public): ', len(dataset_public))\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "    elif args.dataset == 'cifar':\n",
    "        trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        dataset_train = datasets.CIFAR10('data/cifar', train = True, download = False, transform = trans_cifar)\n",
    "        dataset_test = datasets.CIFAR10('data/cifar', train = False, download = False, transform = trans_cifar)\n",
    "\n",
    "        dataset_train, dataset_public = partition_public(dataset_train, args.public_data_ratio)\n",
    "        print('len(dataset_public): ', len(dataset_public))\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "\n",
    "    elif args.dataset == 'cinic':\n",
    "        cinic_mean = [0.47889522, 0.47227842, 0.43047404]\n",
    "        cinic_std = [0.24205776, 0.23828046, 0.25874835]\n",
    "        transform_cinic = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cinic_mean, std=cinic_std)\n",
    "        ])\n",
    "        cinic_directory = 'data/cinic'\n",
    "        dataset_train = datasets.ImageFolder(\n",
    "            os.path.join(cinic_directory, 'train'),\n",
    "            transform=transform_cinic\n",
    "        )\n",
    "        dataset_valid = datasets.ImageFolder(\n",
    "            os.path.join(cinic_directory, 'valid'),\n",
    "            transform=transform_cinic\n",
    "        )\n",
    "        dataset_test = datasets.ImageFolder(\n",
    "            os.path.join(cinic_directory, 'test'),\n",
    "            transform=transform_cinic\n",
    "        )\n",
    "        dataset_train = torch.utils.data.ConcatDataset([dataset_train, dataset_valid])\n",
    "\n",
    "        dataset_train, dataset_public = partition_public(dataset_train, args.public_data_ratio)\n",
    "        print('len(dataset_public): ', len(dataset_public))\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "\n",
    "    print(\"num_users:\", len(dict_users))\n",
    "    img_size = dataset_train[0][0].shape\n",
    "    print(img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model_init = {}\n",
    "acc_init_test = []\n",
    "for x in range(10):\n",
    "    if x % 3 == 0:\n",
    "        model_init[x] = resnet8(10).to(args.device)\n",
    "        model_init[x].eval()\n",
    "        acc_test = test_img(model_init[x], dataset_test, args)\n",
    "        print(\"user-uid:\", x, \"init_Local_Training_accuracy: {:.2f}\".format(acc_test))\n",
    "    elif x % 3 == 1:\n",
    "        model_init[x] = resnet16(10).to(args.device)\n",
    "        model_init[x].eval()\n",
    "        acc_test = test_img(model_init[x], dataset_test, args)\n",
    "        print(\"user-uid:\", x, \"init_Local_Training_accuracy: {:.2f}\".format(acc_test))\n",
    "    else:\n",
    "        model_init[x] = resnet20(10).to(args.device)\n",
    "        model_init[x].eval()\n",
    "        acc_test = test_img(model_init[x], dataset_test, args)\n",
    "        print(\"user-uid:\", x, \"init_Local_Training_accuracy: {:.2f}\".format(acc_test))\n",
    "    acc_init_test.append(acc_test.item())\n",
    "print(\"mean AccTop1 on all clients:\",float(np.mean(np.array(acc_init_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy init_model_parameters to model\n",
    "model = {}\n",
    "for i in range(10):\n",
    "    model[i] = copy.deepcopy(model_init[i])\n",
    "    print(\"---------------------------------model[\", i, \"]---------------------------------\")\n",
    "    print(model[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy init_model_parameters to model_inter\n",
    "model_inter = {}\n",
    "for uid in dict_users:\n",
    "     model_inter[uid] = copy.deepcopy(model_init[uid])\n",
    "\n",
    "for epoch_index in range(args.epochs):       # args.epochs\n",
    "     acc_all=[]\n",
    "     for batch_idx, (images, _) in enumerate(DataLoader(dataset_public, batch_size = 8)):\n",
    "          linear_output_list = []\n",
    "          linear_output_target_list = []          # Save other participants' linear output\n",
    "          mse_loss_batch_list = []\n",
    "          images = images.to(args.device)\n",
    "          '''\n",
    "            Calculate Linear Output\n",
    "          '''\n",
    "          for uid in dict_users:\n",
    "               model[uid] = model[uid].to(args.device)\n",
    "               model[uid].train()\n",
    "               linear_output = model[uid](images)\n",
    "               linear_output_target_list.append(linear_output.clone().detach().cpu().numpy().tolist())\n",
    "               linear_output_list.append(linear_output)\n",
    "          linear_output_target_mean = np.mean(linear_output_target_list,axis = 0)\n",
    "          linear_output_target_mean = torch.tensor(linear_output_target_mean).float().to(args.device)\n",
    "          '''\n",
    "            Update Participants' Models via MAE Loss\n",
    "          '''\n",
    "          for uid in dict_users:\n",
    "               model[uid] = model[uid].to(args.device)\n",
    "               model[uid].train()\n",
    "               criterion = nn.L1Loss(reduction='mean').to(args.device)\n",
    "               optimizer = torch.optim.SGD(model[uid].parameters(), lr = args.lr, momentum = args.momentum, weight_decay = 5e-4)\n",
    "               optimizer.zero_grad()\n",
    "               loss =criterion(linear_output_list[uid], linear_output_target_mean)\n",
    "               mse_loss_batch_list.append(loss.item())\n",
    "               loss.backward()\n",
    "               optimizer.step()\n",
    "\n",
    "     '''\n",
    "       Update Participants' Models via Private Data\n",
    "     '''\n",
    "     local_loss_batch_list = []\n",
    "     for uid in range(len(dict_users)):\n",
    "          model[uid] = model[uid].to(args.device)\n",
    "          model[uid].train()\n",
    "          criterion = nn.CrossEntropyLoss()\n",
    "          optimizer = torch.optim.SGD(model[uid].parameters(), lr = args.lr, momentum = args.momentum, weight_decay = 5e-4)\n",
    "          participant_local_loss_batch_list = []\n",
    "          # iterator = tqdm(range(5))\n",
    "          ldr_train = DataLoader(DatasetSplit(dataset_train, dict_users[uid]), batch_size = 8)\n",
    "          for _ in range(5):\n",
    "               for batch_idx, (images, labels) in enumerate(ldr_train):\n",
    "                    images = images.to(args.device)\n",
    "                    labels = labels.to(args.device)\n",
    "                    outputs = model[uid](images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    optimizer.zero_grad()\n",
    "                    participant_local_loss_batch_list.append(loss.item())\n",
    "                    loss.backward()\n",
    "                    # iterator.desc = \"Local Pariticipant %d loss = %0.3f\" % (uid, loss.item())\n",
    "                    optimizer.step()\n",
    "                    \n",
    "          model_inter[uid] = copy.deepcopy(model[uid])\n",
    "          model[uid].eval()\n",
    "          acc_fine_test = test_img(model[uid], dataset_test, args)\n",
    "          print(\"round:\",epoch_index,\"uid:\",uid,\"Train Testing accuracy: {:.2f}\".format(acc_fine_test))\n",
    "          acc_all.append(acc_fine_test.item())\n",
    "          \n",
    "     print(\"mean Fine_Test/AccTop1 on all clients:\",float(np.mean(np.array(acc_all))))\n",
    "     args.lr = args.lr * (1 - epoch_index / args.epochs * 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
