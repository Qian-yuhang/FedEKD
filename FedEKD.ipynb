{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np   \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from utils.sampling import partition_public, partition_data_dataset\n",
    "from utils.options import args_parser\n",
    "from models.Update import DatasetSplit, CustomDatasetSplit\n",
    "from models.test import test_img\n",
    "from DPKT import calculate_beta, KnowledgeBuffer, importance_sampling, predict, LDP, KnowledgeBuffer, storage\n",
    "from models.resnet_client import resnet20, resnet16, resnet8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda: True\n",
      "Namespace(alpha=10.0, bs=1024, cache_size=160, combined_ep=4, dataset='mnist', device=device(type='cuda', index=0), epochs=50, epsilon=100.0, gpu=0, local_ep=5, lr=0.01, m_num=2, momentum=0.9, num_classes=10, num_users=10, public_data_ratio=0.1, seed=1, train_bs=8)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # parse args\n",
    "    args = args_parser(args=['--epsilon','100', '--m_num','2', '--cache_size','160', '--dataset','mnist', '--momentum','0.9', '--alpha','10', \n",
    "                             '--epochs','50', '--gpu','0', '--public_data_ratio','0.1', '--lr','0.01'])\n",
    "\n",
    "    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "    print('torch.cuda:',torch.cuda.is_available())\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9626406528772272\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    beta = calculate_beta(args.epsilon, args.m_num, args.num_classes, args.num_users)\n",
    "    knowledge_buffer = KnowledgeBuffer(args.cache_size)\n",
    "    print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset_public):  6000\n",
      "len(dataset_train):  54000\n",
      "len(dataset_test):  10000\n",
      "N = 54000\n",
      "num_users: 10\n",
      "torch.Size([3, 28, 28])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# load dataset and split users\n",
    "if __name__ == '__main__':\n",
    "    if args.dataset == 'mnist':\n",
    "        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda img: img.expand(3, -1, -1)), \n",
    "                                                    transforms.Normalize((0.1307,0.1307,0.1307), (0.3081,0.3081,0.3081))])\n",
    "        dataset_train = datasets.MNIST('data/mnist/', train = True, download = False, transform = trans_mnist)\n",
    "        dataset_test = datasets.MNIST('data/mnist/', train = False, download = False, transform = trans_mnist)\n",
    "\n",
    "        dataset_train, dataset_public = partition_public(dataset_train, args.public_data_ratio)\n",
    "        print('len(dataset_public): ', len(dataset_public))\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "    elif args.dataset == 'fashionmnist':\n",
    "        trans_fashionmnist = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda img: img.expand(3, -1, -1)), \n",
    "                                                    transforms.Normalize((0.1307,0.1307,0.1307), (0.3081,0.3081,0.3081))])\n",
    "        dataset_train = datasets.FashionMNIST('data/fashionmnist/', train = True, download = False, transform = trans_fashionmnist)\n",
    "        dataset_test = datasets.FashionMNIST('data/fashionmnist/', train = False, download = False, transform = trans_fashionmnist)\n",
    "    \n",
    "        dataset_train, dataset_public = partition_public(dataset_train, args.public_data_ratio)\n",
    "        print('len(dataset_public): ', len(dataset_public))\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "    elif args.dataset == 'cifar':\n",
    "        trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        dataset_train = datasets.CIFAR10('data/cifar', train = True, download = False, transform = trans_cifar)\n",
    "        dataset_test = datasets.CIFAR10('data/cifar', train = False, download = False, transform = trans_cifar)\n",
    "\n",
    "        dataset_train, dataset_public = partition_public(dataset_train, args.public_data_ratio)\n",
    "        print('len(dataset_public): ', len(dataset_public))\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "\n",
    "    elif args.dataset == 'cinic':\n",
    "        cinic_mean = [0.47889522, 0.47227842, 0.43047404]\n",
    "        cinic_std = [0.24205776, 0.23828046, 0.25874835]\n",
    "        transform_cinic = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cinic_mean, std=cinic_std)\n",
    "        ])\n",
    "        cinic_directory = 'data/cinic'\n",
    "        dataset_train = datasets.ImageFolder(\n",
    "            os.path.join(cinic_directory, 'train'),\n",
    "            transform=transform_cinic\n",
    "        )\n",
    "        dataset_valid = datasets.ImageFolder(\n",
    "            os.path.join(cinic_directory, 'valid'),\n",
    "            transform=transform_cinic\n",
    "        )\n",
    "        dataset_test = datasets.ImageFolder(\n",
    "            os.path.join(cinic_directory, 'test'),\n",
    "            transform=transform_cinic\n",
    "        )\n",
    "        dataset_train = torch.utils.data.ConcatDataset([dataset_train, dataset_valid])\n",
    "\n",
    "        dataset_train, dataset_public = partition_public(dataset_train, args.public_data_ratio)\n",
    "        print('len(dataset_public): ', len(dataset_public))\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "\n",
    "    print(\"num_users:\", len(dict_users))\n",
    "    img_size = dataset_train[0][0].shape\n",
    "    print(img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Distribution of Public Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "label_distribution = [[] for _ in range(10)]\n",
    "for c_id, (images,labels) in enumerate(dataset_public):\n",
    "    label_distribution[labels].append(c_id)\n",
    "\n",
    "list = []\n",
    "for i in range(10):\n",
    "    list.append(len(label_distribution[i]))\n",
    "print(list)\n",
    "\n",
    "x=np.arange(10)\n",
    "y=np.array(list)\n",
    "\n",
    "plt.title(\"Data Distribution of Public Dataset\")\n",
    "plt.bar(x,y,tick_label=[c_id for c_id in range(10)], width = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Distribution of Different Clients\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "label_distribution = [[] for _ in range(10)]\n",
    "for c_id in dict_users:\n",
    "    for idx in dict_users[c_id]:\n",
    "        label_distribution[dataset_train[idx][1]].append(c_id)\n",
    "\n",
    "plt.hist(label_distribution, stacked = True,\n",
    "            bins = np.arange(-0.5, 10 + 1.5, 1),\n",
    "            label = np.arange(10), rwidth = 0.8)\n",
    "plt.xticks(np.arange(10), [\"Client %d\" %\n",
    "                                    c_id for c_id in range(10)])\n",
    "plt.xlabel(\"Client ID\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.legend()\n",
    "plt.title(\"Label Distribution of Different Clients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model_init = {}\n",
    "acc_init_test = []\n",
    "for x in range(10):\n",
    "    if x % 3 == 0:\n",
    "        model_init[x] = resnet8(10).to(args.device)\n",
    "        model_init[x].eval()\n",
    "        acc_test = test_img(model_init[x], dataset_test, args)\n",
    "        print(\"user-uid:\", x, \"init_Local_Training_accuracy: {:.2f}\".format(acc_test))\n",
    "    elif x % 3 == 1:\n",
    "        model_init[x] = resnet16(10).to(args.device)\n",
    "        model_init[x].eval()\n",
    "        acc_test = test_img(model_init[x], dataset_test, args)\n",
    "        print(\"user-uid:\", x, \"init_Local_Training_accuracy: {:.2f}\".format(acc_test))\n",
    "    else:\n",
    "        model_init[x] = resnet20(10).to(args.device)\n",
    "        model_init[x].eval()\n",
    "        acc_test = test_img(model_init[x], dataset_test, args)\n",
    "        print(\"user-uid:\", x, \"init_Local_Training_accuracy: {:.2f}\".format(acc_test))\n",
    "    acc_init_test.append(acc_test.item())\n",
    "print(\"mean AccTop1 on all clients:\",float(np.mean(np.array(acc_init_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy init_model_parameters\n",
    "model = {}\n",
    "for i in range(10):\n",
    "    model[i] = copy.deepcopy(model_init[i])\n",
    "    print(\"---------------------------------model[\", i, \"]---------------------------------\")\n",
    "    print(model[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total_params\n",
    "total_params = sum(p.numel() for p in model[0].parameters())\n",
    "\n",
    "# calculate trainable_params\n",
    "trainable_params = sum(p.numel() for p in model[0].parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-train\n",
    "for uid in dict_users:\n",
    "    model[uid].to(args.device)\n",
    "    model[uid].train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model[uid].parameters(), lr = args.lr, momentum = args.momentum, weight_decay = 5e-4)\n",
    "    ldr_train = DataLoader(DatasetSplit(dataset_train, dict_users[uid]), batch_size = 512, shuffle = True)\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(ldr_train):\n",
    "        images, labels = images.to(args.device), labels.to(args.device)\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model[uid](images)\n",
    "        loss = criterion(log_probs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "data_rounds = []\n",
    "\n",
    "for round in range(args.epochs):\n",
    "\n",
    "    acc_all_local=[]\n",
    "    acc_all=[]\n",
    "\n",
    "    '''\n",
    "      Ensemble Knowledge Acquisition\n",
    "    '''\n",
    "    knowledge_transfer_data = []\n",
    "    for uid in dict_users:\n",
    "        transfer_data = importance_sampling(args.m_num, model[uid], dataset_public, args)\n",
    "        knowledge_transfer_data.append(transfer_data)           # list\n",
    "    knowledge_transfer_data = [item for sublist in knowledge_transfer_data for item in sublist]\n",
    "    knowledge_transfer_data = [tensor.item() for tensor in knowledge_transfer_data]\n",
    "\n",
    "    transfer_data_index = []\n",
    "    for element in knowledge_transfer_data:\n",
    "        transfer_data_index.append(element)\n",
    "    transfer_data_index = np.array(transfer_data_index)           # np.array() [2010 2460 8597 9544 10616 3829 251 5537 6092 1105]\n",
    "\n",
    "    # updata perturbed results and entropy information\n",
    "    uploaded_perturbed_predictions = []\n",
    "    uploaded_entropy_information = []\n",
    "\n",
    "    # pre_model_parameters\n",
    "    model_pre = {}\n",
    "    for uid in dict_users:\n",
    "        model_pre[uid] = copy.deepcopy(model[uid])\n",
    "\n",
    "    '''\n",
    "      Local_Training\n",
    "    '''\n",
    "    for uid in dict_users:\n",
    "        model[uid].to(args.device)\n",
    "        model[uid].train()\n",
    "        criterionCE = nn.CrossEntropyLoss()\n",
    "        criterionKL = nn.KLDivLoss(reduction='batchmean')\n",
    "        optimizer = torch.optim.SGD(model[uid].parameters(), lr = args.lr, momentum = args.momentum, weight_decay = 5e-4)\n",
    "        ldr_train = DataLoader(DatasetSplit(dataset_train, dict_users[uid]), batch_size = 8, shuffle = True)\n",
    "        for _ in range(args.local_ep):\n",
    "            for batch_idx, (images, labels) in enumerate(ldr_train):\n",
    "                images, labels = images.to(args.device), labels.to(args.device)\n",
    "                probs = model[uid](images)\n",
    "                log_probs = F.log_softmax(probs, dim = 1)\n",
    "                pre_probs = F.softmax(model_pre[uid](images), dim = 1)\n",
    "                loss_hard = criterionCE(probs, labels)\n",
    "                loss_pri = criterionKL(log_probs, pre_probs)\n",
    "                loss = loss_hard + (1 + 0.04 * (round + 1)) * loss_pri\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Computing local model predictions and information entropy\n",
    "        knowledge_transfer_images = []\n",
    "        for idx in transfer_data_index:\n",
    "            image = dataset_public[idx]\n",
    "            knowledge_transfer_images.append(image)\n",
    "        local_predictions, entropy_information = predict(model[uid], knowledge_transfer_images, uid, args)                           # local_predictions、entropy_information\n",
    "\n",
    "        # Perturb local model predictions\n",
    "        perturbed_local_predictions = LDP(local_predictions, beta, args.num_classes, args).tolist()                   # one-hot\n",
    "        uploaded_perturbed_predictions.append(perturbed_local_predictions)\n",
    "        uploaded_entropy_information.append(entropy_information.tolist())\n",
    "\n",
    "    uploaded_perturbed_predictions = np.array(uploaded_perturbed_predictions, dtype = float)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "      Entropy-Aware Aggregation\n",
    "    '''\n",
    "    entropies_array = np.array(uploaded_entropy_information)\n",
    "    sorted_indices = np.argsort(entropies_array, axis = 0)\n",
    "    k = 7\n",
    "    top_k_indices = sorted_indices[:k, :]\n",
    "    average_predictions = []\n",
    "    for i in range(len(uploaded_perturbed_predictions[0])):\n",
    "        top_k_predictions = [uploaded_perturbed_predictions[j][i] for j in top_k_indices[:, i]]\n",
    "        average_prediction = np.mean(top_k_predictions, axis = 0)\n",
    "        average_predictions.append(average_prediction)\n",
    "    average_predictions = np.array(average_predictions)\n",
    "\n",
    "    # Knolwedge aggergation\n",
    "    # uploaded_perturbed_predictions.mean(axis = 0)\n",
    "    aggregated_predictions = (average_predictions - (1 - beta) / args.num_classes) / beta\n",
    "    aggregated_predictions = torch.tensor(aggregated_predictions)\n",
    "    aggregated_predictions = torch.argmax(aggregated_predictions, dim = 1)\n",
    "\n",
    "    # Store distillation knowledge data in the distillation knowledge cache\n",
    "    Fine_tuning_knowledge_transfer_data, Fine_tuning_aggregated_predictions = storage(transfer_data_index, aggregated_predictions, knowledge_buffer)\n",
    "    Fine_tuning_knowledge_transfer_data = Fine_tuning_knowledge_transfer_data.tolist()\n",
    "    Fine_tuning_aggregated_predictions = Fine_tuning_aggregated_predictions.tolist()\n",
    "\n",
    "    # pre_model_parameters\n",
    "    model_local = {}\n",
    "    for uid in dict_users:\n",
    "        model_local[uid] = copy.deepcopy(model[uid])\n",
    "    \n",
    "\n",
    "    labels = []\n",
    "    for i in Fine_tuning_knowledge_transfer_data:\n",
    "        labels.append(dataset_public[i][1])\n",
    "    data_rounds.append(labels)\n",
    "\n",
    "    '''\n",
    "      Combined_Distillation_Training\n",
    "    '''\n",
    "    for uid in dict_users:\n",
    "        # Local_Training\n",
    "        model[uid].to(args.device)\n",
    "        model[uid].train()\n",
    "        Fine_criterion = nn.CrossEntropyLoss()\n",
    "        criterionKL = nn.KLDivLoss(reduction = 'batchmean')\n",
    "        Fine_optimizer = torch.optim.SGD(model[uid].parameters(), lr = args.lr, momentum = args.momentum, weight_decay = 5e-4)\n",
    "        Fine_train = DataLoader(CustomDatasetSplit(dataset_public, Fine_tuning_knowledge_transfer_data, Fine_tuning_aggregated_predictions), \n",
    "                                    batch_size = 8, shuffle = True)\n",
    "        for _ in range(args.combined_ep):\n",
    "            for batch_idx, (images, labels) in enumerate(Fine_train):\n",
    "                images, labels = images.to(args.device), labels.to(args.device)\n",
    "                probs = model[uid](images)\n",
    "                log_probs = F.log_softmax(probs, dim = 1)\n",
    "                local_probs = F.softmax(model_local[uid](images), dim = 1)\n",
    "                loss_local_hard = Fine_criterion(probs, labels)\n",
    "                loss_local_pri = criterionKL(log_probs, local_probs)\n",
    "                loss_local = 1 * loss_local_hard + 3 * loss_local_pri\n",
    "                Fine_optimizer.zero_grad()\n",
    "                loss_local.backward()\n",
    "                Fine_optimizer.step()                         \n",
    "        \n",
    "        model[uid].eval()\n",
    "        acc_fine_test = test_img(model[uid], dataset_test, args)\n",
    "        # print(\"round:\",round,\"uid:\",uid,\"Fine_Train Testing accuracy: {:.2f}\".format(acc_fine_test))\n",
    "        acc_all.append(acc_fine_test.item())\n",
    "    \n",
    "    args.lr = args.lr * (1 - round / args.epochs * 0.9)\n",
    "    print(round + 1, \":\", \"mean AccTop1 on all clients:\", float(np.mean(np.array(acc_all))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the image of the n-th sample\n",
    "image, label = dataset_public[3135]\n",
    "\n",
    "image_np = image.numpy()\n",
    "image_np = image_np.transpose(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(image_np)\n",
    "plt.title(f'Label: {label}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the image of the n-th sample\n",
    "image, label = dataset_public[2473]\n",
    "\n",
    "image = image[0]\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(f'Label: {label}')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
