{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np   \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "from utils.sampling import partition_public, partition_data_dataset\n",
    "from utils.options import args_parser\n",
    "from models.Update import DatasetSplit\n",
    "from models.test import test_img\n",
    "from models.resnet_client import resnet20, resnet16, resnet8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # parse args\n",
    "    args = args_parser(args=['--dataset','mnist', '--momentum','0.9', '--alpha','1',\n",
    "                             '--epochs','50', '--gpu','0', '--public_data_ratio','0.1', '--lr','0.01'])\n",
    "\n",
    "    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "    print('torch.cuda:',torch.cuda.is_available())\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and split users\n",
    "if __name__ == '__main__':\n",
    "    if args.dataset == 'mnist':\n",
    "        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda img: img.expand(3, -1, -1)), \n",
    "                                                    transforms.Normalize((0.1307,0.1307,0.1307), (0.3081,0.3081,0.3081))])\n",
    "        dataset_train = datasets.MNIST('data/mnist/', train = True, download = False, transform = trans_mnist)\n",
    "        dataset_test = datasets.MNIST('data/mnist/', train = False, download = False, transform = trans_mnist)\n",
    "\n",
    "        dataset_train, dataset_public = partition_public(dataset_train, args.public_data_ratio)\n",
    "        print('len(dataset_public): ', len(dataset_public))\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "    elif args.dataset == 'fashionmnist':\n",
    "        trans_fashionmnist = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda img: img.expand(3, -1, -1)), \n",
    "                                                    transforms.Normalize((0.1307,0.1307,0.1307), (0.3081,0.3081,0.3081))])\n",
    "        dataset_train = datasets.FashionMNIST('data/fashionmnist/', train = True, download = False, transform = trans_fashionmnist)\n",
    "        dataset_test = datasets.FashionMNIST('data/fashionmnist/', train = False, download = False, transform = trans_fashionmnist)\n",
    "    \n",
    "        dataset_train, dataset_public = partition_public(dataset_train, args.public_data_ratio)\n",
    "        print('len(dataset_public): ', len(dataset_public))\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "    elif args.dataset == 'cifar':\n",
    "        trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        dataset_train = datasets.CIFAR10('data/cifar', train = True, download = False, transform = trans_cifar)\n",
    "        dataset_test = datasets.CIFAR10('data/cifar', train = False, download = False, transform = trans_cifar)\n",
    "\n",
    "        dataset_train, dataset_public = partition_public(dataset_train, args.public_data_ratio)\n",
    "        print('len(dataset_public): ', len(dataset_public))\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "\n",
    "    elif args.dataset == 'cinic':\n",
    "        cinic_mean = [0.47889522, 0.47227842, 0.43047404]\n",
    "        cinic_std = [0.24205776, 0.23828046, 0.25874835]\n",
    "        transform_cinic = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cinic_mean, std=cinic_std)\n",
    "        ])\n",
    "        cinic_directory = 'data/cinic'\n",
    "        dataset_train = datasets.ImageFolder(\n",
    "            os.path.join(cinic_directory, 'train'),\n",
    "            transform=transform_cinic\n",
    "        )\n",
    "        dataset_valid = datasets.ImageFolder(\n",
    "            os.path.join(cinic_directory, 'valid'),\n",
    "            transform=transform_cinic\n",
    "        )\n",
    "        dataset_test = datasets.ImageFolder(\n",
    "            os.path.join(cinic_directory, 'test'),\n",
    "            transform=transform_cinic\n",
    "        )\n",
    "        dataset_train = torch.utils.data.ConcatDataset([dataset_train, dataset_valid])\n",
    "\n",
    "        dataset_train, dataset_public = partition_public(dataset_train, args.public_data_ratio)\n",
    "        print('len(dataset_public): ', len(dataset_public))\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "\n",
    "    print(\"num_users:\", len(dict_users))\n",
    "    img_size = dataset_train[0][0].shape\n",
    "    print(img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model_init = {}\n",
    "acc_init_test = []\n",
    "for x in range(10):\n",
    "    if x % 3 == 0:\n",
    "        model_init[x] = resnet8(10).to(args.device)\n",
    "        model_init[x].eval()\n",
    "        acc_test = test_img(model_init[x], dataset_test, args)\n",
    "        print(\"user-uid:\", x, \"init_Local_Training_accuracy: {:.2f}\".format(acc_test))\n",
    "    elif x % 3 == 1:\n",
    "        model_init[x] = resnet16(10).to(args.device)\n",
    "        model_init[x].eval()\n",
    "        acc_test = test_img(model_init[x], dataset_test, args)\n",
    "        print(\"user-uid:\", x, \"init_Local_Training_accuracy: {:.2f}\".format(acc_test))\n",
    "    else:\n",
    "        model_init[x] = resnet20(10).to(args.device)\n",
    "        model_init[x].eval()\n",
    "        acc_test = test_img(model_init[x], dataset_test, args)\n",
    "        print(\"user-uid:\", x, \"init_Local_Training_accuracy: {:.2f}\".format(acc_test))\n",
    "    acc_init_test.append(acc_test.item())\n",
    "print(\"mean AccTop1 on all clients:\",float(np.mean(np.array(acc_init_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy init_model_parameters to model\n",
    "model = {}\n",
    "for i in range(10):\n",
    "    model[i] = copy.deepcopy(model_init[i])\n",
    "    print(\"---------------------------------model[\", i, \"]---------------------------------\")\n",
    "    print(model[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy init_model_parameters to model_intra\n",
    "model_intra = {}\n",
    "for i in range(10):\n",
    "    model_intra[i] = copy.deepcopy(model_init[i])\n",
    "    print(\"---------------------------------model_intra[\", i, \"]---------------------------------\")\n",
    "    print(model_intra[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  pretrain-intra_net\n",
    "'''\n",
    "for uid in dict_users:\n",
    "    model_intra[uid] = model_intra[uid].to(args.device)\n",
    "    model_intra[uid].train()\n",
    "    optimizer = torch.optim.Adam(model_intra[uid].parameters(), lr = 0.01)\n",
    "    scheduler = CosineLRScheduler(optimizer, t_initial = 50, lr_min = 1e-6)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion.to(args.device)\n",
    "    iterator = tqdm(range(50))\n",
    "    ldr_train = DataLoader(DatasetSplit(dataset_train, dict_users[uid]), batch_size = 128)\n",
    "    for epoch_index in iterator:\n",
    "        for batch_idx, (images, labels) in enumerate(ldr_train):\n",
    "            images = images.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            outputs = model_intra[uid](images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            iterator.desc = \"Local Pariticipant %d loss = %0.3f\" % (uid,loss)\n",
    "            optimizer.step()\n",
    "        model_intra[uid].eval()\n",
    "        acc_test = test_img(model_intra[uid], dataset_test, args)\n",
    "        if epoch_index == 49:\n",
    "          print(\"round:\",epoch_index,\"uid:\",uid,\"Pretrain-intra_net Testing Accuracy: {:.2f}\".format(acc_test))\n",
    "        scheduler.step(epoch_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_accs_dict = {}\n",
    "inter_accs_dict = {}\n",
    "mean_intra_acc_list = []\n",
    "mean_inter_acc_list = []\n",
    "\n",
    "def _off_diagonal(x):\n",
    "     n, m = x.shape\n",
    "     assert n == m\n",
    "     return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "# copy init_model_parameters to model_inter\n",
    "model_inter = {}\n",
    "for uid in dict_users:\n",
    "     model_inter[uid] = copy.deepcopy(model_init[uid])\n",
    "\n",
    "for epoch_index in range(args.epochs):       # args.epochs\n",
    "     for batch_idx, (images, _) in enumerate(DataLoader(dataset_public, batch_size = 8)):\n",
    "          '''\n",
    "            Aggregate the output from participants\n",
    "          '''\n",
    "          linear_output_list = []                         # Store the raw output of each model\n",
    "          linear_output_target_list = []                  # Store a copy of each model output\n",
    "          images = images.to(args.device)\n",
    "          \n",
    "          for uid in dict_users:\n",
    "               model[uid] = model[uid].to(args.device)\n",
    "               model[uid].train()\n",
    "               linear_output  = model[uid](images)\n",
    "               linear_output_target_list.append(linear_output.clone().detach())\n",
    "               linear_output_list.append(linear_output)\n",
    "\n",
    "          '''\n",
    "            Update Participants' Models via Col Loss\n",
    "          '''\n",
    "          for uid in dict_users:\n",
    "               model[uid] = model[uid].to(args.device)\n",
    "               model[uid].train()\n",
    "               optimizer = torch.optim.SGD(model[uid].parameters(), lr = args.lr, momentum = args.momentum, weight_decay = 5e-4)\n",
    "               linear_output_target_avg_list = []         # Store output copies for all participants\n",
    "               for k in range(len(dict_users)):\n",
    "                    linear_output_target_avg_list.append(linear_output_target_list[k])         # Add each participant's output copy to the list\n",
    "\n",
    "               linear_output_target_avg = torch.mean(torch.stack(linear_output_target_avg_list), 0)\n",
    "               linear_output = linear_output_list[uid]\n",
    "               z_1_bn = (linear_output-linear_output.mean(0)) / linear_output.std(0)\n",
    "               # Standardize the output z_1-bn of the current model and the average value z_2-bn of the target output (subtract the mean and divide by the standard deviation)\n",
    "               z_2_bn = (linear_output_target_avg - linear_output_target_avg.mean(0)) / linear_output_target_avg.std(0)\n",
    "               # Calculate the comparison matrix c between the standardized outputs and implement it through matrix multiplication\n",
    "               c = z_1_bn.T @ z_2_bn\n",
    "               # normalization\n",
    "               c.div_(len(images))\n",
    "\n",
    "               on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()        # Loss of diagonal elements\n",
    "               off_diag = _off_diagonal(c).add_(1).pow_(2).sum()         # Loss of non-diagonal elements\n",
    "               optimizer.zero_grad()\n",
    "               col_loss = on_diag + 0.0051 * off_diag\n",
    "               col_loss.backward()\n",
    "               optimizer.step()\n",
    "\n",
    "\n",
    "     for uid in dict_users:\n",
    "          model[uid] = model[uid].to(args.device)\n",
    "          model_inter[uid] = model_inter[uid].to(args.device)\n",
    "          model_intra[uid] = model_intra[uid].to(args.device)\n",
    "          optimizer = torch.optim.SGD(model[uid].parameters(), lr = args.lr, momentum = args.momentum, weight_decay = 5e-4)\n",
    "          criterionCE = nn.CrossEntropyLoss()\n",
    "          criterionCE.to(args.device)\n",
    "          criterionKL = nn.KLDivLoss(reduction='batchmean')\n",
    "          criterionKL.to(args.device)\n",
    "          ldr_train = DataLoader(DatasetSplit(dataset_train, dict_users[uid]), batch_size = 8)\n",
    "          for _ in range(5):\n",
    "               for batch_idx, (images, labels) in enumerate(ldr_train):\n",
    "                    images = images.to(args.device)\n",
    "                    labels = labels.to(args.device)\n",
    "                    outputs = model[uid](images)\n",
    "                    logsoft_outputs = F.log_softmax(outputs, dim=1)\n",
    "                    with torch.no_grad():\n",
    "                         intra_soft_outpus = F.softmax(model_intra[uid](images), dim=1)\n",
    "                         inter_soft_outpus = F.softmax(model_inter[uid](images), dim=1)\n",
    "                    intra_loss = criterionKL(logsoft_outputs, intra_soft_outpus)\n",
    "                    inter_loss = criterionKL(logsoft_outputs, inter_soft_outpus)\n",
    "                    loss_hard = criterionCE(outputs, labels)\n",
    "                    loss = loss_hard + inter_loss + intra_loss\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "          model_inter[uid] = copy.deepcopy(model[uid])\n",
    "          model[uid].eval()\n",
    "          acc_fine_test = test_img(model[uid], dataset_test, args)\n",
    "          print(\"round:\",epoch_index,\"uid:\",uid,\"Train Testing accuracy: {:.2f}\".format(acc_fine_test))\n",
    "     args.lr = args.lr * (1 - epoch_index / args.epochs * 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
