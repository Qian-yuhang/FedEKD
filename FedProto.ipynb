{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np   \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from utils.sampling import partition_data_dataset\n",
    "from utils.options import args_parser\n",
    "from models.Update import DatasetSplit\n",
    "from models.test import test_img\n",
    "from models.resnet_client import resnet20, resnet16, resnet8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # parse args\n",
    "    args = args_parser(args=['--dataset','cinic', '--momentum','0.9', '--alpha','1',\n",
    "                             '--epochs','50', '--gpu','0', '--lr','0.01'])\n",
    "\n",
    "    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "    print('torch.cuda:',torch.cuda.is_available())\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and split users\n",
    "# No Public Data Partition\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if args.dataset == 'mnist':\n",
    "        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda img: img.expand(3, -1, -1)), \n",
    "                                                    transforms.Normalize((0.1307, 0.1307, 0.1307), (0.3081, 0.3081, 0.3081))])\n",
    "        dataset_train = datasets.MNIST('data/mnist/', train = True, download = False, transform = trans_mnist)\n",
    "        dataset_test = datasets.MNIST('data/mnist/', train = False, download = False, transform = trans_mnist)\n",
    "\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "    elif args.dataset == 'fashionmnist':\n",
    "        trans_fashionmnist = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda img: img.expand(3, -1, -1)), \n",
    "                                                    transforms.Normalize((0.1307, 0.1307, 0.1307), (0.3081, 0.3081, 0.3081))])\n",
    "        dataset_train = datasets.FashionMNIST('data/fashionmnist/', train = True, download = False, transform = trans_fashionmnist)\n",
    "        dataset_test = datasets.FashionMNIST('data/fashionmnist/', train = False, download = False, transform = trans_fashionmnist)\n",
    "    \n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "    elif args.dataset == 'cifar':\n",
    "        trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        dataset_train = datasets.CIFAR10('data/cifar', train = True, download = False, transform = trans_cifar)\n",
    "        dataset_test = datasets.CIFAR10('data/cifar', train = False, download = False, transform = trans_cifar)\n",
    "\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "\n",
    "    elif args.dataset == 'cinic':\n",
    "        cinic_mean = [0.47889522, 0.47227842, 0.43047404]\n",
    "        cinic_std = [0.24205776, 0.23828046, 0.25874835]\n",
    "        transform_cinic = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cinic_mean, std=cinic_std)\n",
    "        ])\n",
    "        cinic_directory = 'data/cinic'\n",
    "        dataset_train = datasets.ImageFolder(\n",
    "            os.path.join(cinic_directory, 'train'),\n",
    "            transform=transform_cinic\n",
    "        )\n",
    "        dataset_valid = datasets.ImageFolder(\n",
    "            os.path.join(cinic_directory, 'valid'),\n",
    "            transform=transform_cinic\n",
    "        )\n",
    "        dataset_test = datasets.ImageFolder(\n",
    "            os.path.join(cinic_directory, 'test'),\n",
    "            transform=transform_cinic\n",
    "        )\n",
    "        dataset_train = torch.utils.data.ConcatDataset([dataset_train, dataset_valid])\n",
    "\n",
    "\n",
    "        print('len(dataset_train): ', len(dataset_train))\n",
    "        print('len(dataset_test): ', len(dataset_test))\n",
    "        \n",
    "        dataset_train_labels = np.array([])\n",
    "        for i,(x, y) in enumerate(dataset_train):\n",
    "            dataset_train_labels = np.append(dataset_train_labels, y)\n",
    "        dataset_train_labels = dataset_train_labels.astype(int)\n",
    "\n",
    "        dict_users = partition_data_dataset(dataset_train_labels, 10, alpha = args.alpha)\n",
    "\n",
    "\n",
    "    print(\"num_users:\", len(dict_users))\n",
    "    img_size = dataset_train[0][0].shape\n",
    "    print(img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model_init = {}\n",
    "acc_init_test = []\n",
    "for x in range(10):\n",
    "    if x % 3 == 0:\n",
    "        model_init[x] = resnet8(10).to(args.device)\n",
    "        model_init[x].eval()\n",
    "        acc_test = test_img(model_init[x], dataset_test, args)\n",
    "        print(\"user-uid:\", x, \"init_Local_Training_accuracy: {:.2f}\".format(acc_test))\n",
    "    elif x % 3 == 1:\n",
    "        model_init[x] = resnet16(10).to(args.device)\n",
    "        model_init[x].eval()\n",
    "        acc_test = test_img(model_init[x], dataset_test, args)\n",
    "        print(\"user-uid:\", x, \"init_Local_Training_accuracy: {:.2f}\".format(acc_test))\n",
    "    else:\n",
    "        model_init[x] = resnet20(10).to(args.device)\n",
    "        model_init[x].eval()\n",
    "        acc_test = test_img(model_init[x], dataset_test, args)\n",
    "        print(\"user-uid:\", x, \"init_Local_Training_accuracy: {:.2f}\".format(acc_test))\n",
    "    acc_init_test.append(acc_test.item())\n",
    "print(\"mean AccTop1 on all clients:\",float(np.mean(np.array(acc_init_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy init_model_parameters to model\n",
    "model = {}\n",
    "for i in range(10):\n",
    "    model[i] = copy.deepcopy(model_init[i])\n",
    "    print(\"---------------------------------model[\", i, \"]---------------------------------\")\n",
    "    print(model[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_protos = []\n",
    "idxs_users = np.arange(len(dict_users))\n",
    "train_loss, train_accuracy = [], []\n",
    "\n",
    "def agg_func(protos):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "\n",
    "    for [label, proto_list] in protos.items():\n",
    "        if len(proto_list) > 1:\n",
    "            proto = 0 * proto_list[0].data\n",
    "            for i in proto_list:\n",
    "                proto += i.data\n",
    "            protos[label] = proto / len(proto_list)\n",
    "        else:\n",
    "            protos[label] = proto_list[0]\n",
    "\n",
    "    return protos\n",
    "\n",
    "def proto_aggregation(local_protos_list):\n",
    "    agg_protos_label = dict()\n",
    "    for idx in local_protos_list:\n",
    "        local_protos = local_protos_list[idx]\n",
    "        for label in local_protos.keys():\n",
    "            if label in agg_protos_label:\n",
    "                agg_protos_label[label].append(local_protos[label])\n",
    "            else:\n",
    "                agg_protos_label[label] = [local_protos[label]]\n",
    "\n",
    "    for [label, proto_list] in agg_protos_label.items():\n",
    "        if len(proto_list) > 1:\n",
    "            proto = 0 * proto_list[0].data\n",
    "            for i in proto_list:\n",
    "                proto += i.data\n",
    "            agg_protos_label[label] = [proto / len(proto_list)]\n",
    "        else:\n",
    "            agg_protos_label[label] = [proto_list[0].data]\n",
    "\n",
    "    return agg_protos_label\n",
    "\n",
    "for round in range(1):\n",
    "    \n",
    "     acc_all=[]\n",
    "    \n",
    "     local_weights, local_losses, local_protos = [], [], {}\n",
    "     print(f'\\n | Global Training Round : {round + 1} |\\n')\n",
    "\n",
    "     proto_loss = 0\n",
    "     for idx in range(len(dict_users)):\n",
    "          model[idx].train()\n",
    "          criterionCE = nn.CrossEntropyLoss()\n",
    "          \n",
    "          optimizer = torch.optim.SGD(model[idx].parameters(), lr = args.lr, momentum = args.momentum, weight_decay = 5e-4)\n",
    "          ldr_train = DataLoader(DatasetSplit(dataset_train, dict_users[idx]), batch_size = 8, shuffle = True)\n",
    "          for iter in range(1):         # train_ep = 1\n",
    "               agg_protos_label = {}    # Aggregate local prototypes by category\n",
    "               for batch_idx, (images, label_g) in enumerate(ldr_train):\n",
    "                    images, labels = images.to(args.device), label_g.to(args.device)\n",
    "                    model[idx].zero_grad()\n",
    "                    log_probs = model[idx](images)\n",
    "                    protos = model[idx].features(images)\n",
    "                    loss1 = criterionCE(log_probs, labels)\n",
    "\n",
    "                    loss_mse = nn.MSELoss()\n",
    "                    if len(global_protos) == 0:\n",
    "                         loss2 = 0 * loss1\n",
    "                    else:\n",
    "                         proto_new = copy.deepcopy(protos.data)\n",
    "                         i = 0\n",
    "                         for label in labels:\n",
    "                              if label.item() in global_protos.keys():\n",
    "                                   proto_new[i, :] = global_protos[label.item()][0].data\n",
    "                              i += 1\n",
    "                         loss2 = loss_mse(proto_new, protos)\n",
    "                    loss = loss1 + loss2 * 1\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    for i in range(len(labels)):\n",
    "                         if label_g[i].item() in agg_protos_label:\n",
    "                              agg_protos_label[label_g[i].item()].append(protos[i,:])\n",
    "                         else:\n",
    "                              agg_protos_label[label_g[i].item()] = [protos[i,:]]\n",
    "\n",
    "                    log_probs = log_probs[:, 0:args.num_classes]\n",
    "                    _, y_hat = log_probs.max(1)\n",
    "                    acc_val = torch.eq(y_hat, labels.squeeze()).float().mean()\n",
    "          \n",
    "          print(agg_protos_label)\n",
    "          agg_protos = agg_func(agg_protos_label)\n",
    "          local_protos[idx] = agg_protos\n",
    "\n",
    "          # update global protos\n",
    "          global_protos = proto_aggregation(local_protos)\n",
    "          if idx == 9:\n",
    "            print(global_protos)\n",
    "          model[idx].eval()\n",
    "          acc_fine_test = test_img(model[idx], dataset_test, args)\n",
    "        #   print(\"round:\",round,\"idx:\",idx,\"Train Testing accuracy: {:.2f}\".format(acc_fine_test))\n",
    "          acc_all.append(acc_fine_test.item())\n",
    "    \n",
    "     print(\"mean Fine_Test/AccTop1 on all clients:\",float(np.mean(np.array(acc_all))))\n",
    "     args.lr = args.lr * (1 - round / args.epochs * 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
